{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO00LiqHbR1TNrapBHAwsCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TANJUMAJERIN/Deep-Dive-in-The-Field-of-Misinformation/blob/main/Practice(DP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rmwy535OVEla",
        "outputId": "a7e00498-aa58-4878-bcd7-40831b26953d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello, i am jerin's. Nice to meet you today! hello there.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "corpus= \"hello, i am jerin's. Nice to meet you today! hello there.\"\n",
        "print(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "#paragraph-> sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents=sent_tokenize(corpus)\n",
        "print(documents)"
      ],
      "metadata": {
        "id": "zx2FNg0TZk-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b347b56-82df-4a0a-d4c5-8cd10361e737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"hello, i am jerin's.\", 'Nice to meet you today!', 'hello there.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZixqQcaaLRp",
        "outputId": "32dfd15c-87cc-4bb2-c424-913e8b148bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvyBBlVewph",
        "outputId": "d32b84d9-2c68-4c98-d444-de8722c96ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello, i am jerin's.\n",
            "Nice to meet you today!\n",
            "hello there.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokrnization\n",
        "#para-> words\n",
        "#sentence-> words\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRQjmed1fDDi",
        "outputId": "31ace3af-bd48-4474-e841-0b6dd27334e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " ',',\n",
              " 'i',\n",
              " 'am',\n",
              " 'jerin',\n",
              " \"'s\",\n",
              " '.',\n",
              " 'Nice',\n",
              " 'to',\n",
              " 'meet',\n",
              " 'you',\n",
              " 'today',\n",
              " '!',\n",
              " 'hello',\n",
              " 'there',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYbCF1jsg4SA",
        "outputId": "2826c779-923e-4586-ba11-446026e2369b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ',', 'i', 'am', 'jerin', \"'s\", '.']\n",
            "['Nice', 'to', 'meet', 'you', 'today', '!']\n",
            "['hello', 'there', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5FdmN0Ih-qb",
        "outputId": "2df35890-8439-4af7-acc1-f192aa3cd35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " ',',\n",
              " 'i',\n",
              " 'am',\n",
              " 'jerin',\n",
              " \"'\",\n",
              " 's',\n",
              " '.',\n",
              " 'Nice',\n",
              " 'to',\n",
              " 'meet',\n",
              " 'you',\n",
              " 'today',\n",
              " '!',\n",
              " 'hello',\n",
              " 'there',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer(corpus)\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "654ASihZiMzA",
        "outputId": "6e070bdb-0eab-4aa2-a1a8-4f4a599483fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TreebankWordTokenizer() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-be75a0958f8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TreebankWordTokenizer() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **stemming**"
      ],
      "metadata": {
        "id": "aC6dXeGiK26T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"going\",\"goes\",\"eaten\",\"went\", \"everything\",\"people\",\"notice\",\"finally\",\"finalized\",\"programm\",\"programming\",\"ate\",\"finalizing\",\"history\",\"historical\",\"fairly\",\"strongly\"]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjoTv89oBZTd",
        "outputId": "3878a551-8c22-45a0-a711-afdf9a4289c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['going', 'goes', 'eaten', 'went', 'everything', 'people', 'notice', 'finally', 'finalized', 'programm', 'programming', 'ate', 'finalizing', 'history', 'historical', 'fairly', 'strongly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming=PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+ \"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH4qDC1OBwnl",
        "outputId": "51fede99-dd0e-4833-9ba5-1ca7d7642bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going---->go\n",
            "goes---->goe\n",
            "eaten---->eaten\n",
            "went---->went\n",
            "everything---->everyth\n",
            "people---->peopl\n",
            "notice---->notic\n",
            "finally---->final\n",
            "finalized---->final\n",
            "programm---->programm\n",
            "programming---->program\n",
            "ate---->ate\n",
            "finalizing---->final\n",
            "history---->histori\n",
            "historical---->histor\n",
            "fairly---->fairli\n",
            "strongly---->strongli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexpstemmer=RegexpStemmer('ing|s$|able$',min=4)\n",
        "regexpstemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2-sf9L1BBxe5",
        "outputId": "28283f20-ff05-4d6b-8060-8e885f634b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')\n",
        "for word in words:\n",
        "  print(word+\"---->\"+snowballstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBWzPRMqByH7",
        "outputId": "7c7a1ae7-8fea-4978-d29c-65b650dbad18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going---->go\n",
            "goes---->goe\n",
            "eaten---->eaten\n",
            "went---->went\n",
            "everything---->everyth\n",
            "people---->peopl\n",
            "notice---->notic\n",
            "finally---->final\n",
            "finalized---->final\n",
            "programm---->programm\n",
            "programming---->program\n",
            "ate---->ate\n",
            "finalizing---->final\n",
            "history---->histori\n",
            "historical---->histor\n",
            "fairly---->fair\n",
            "strongly---->strong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "1MzhCWeYKXJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "\n",
        "for word in words:\n",
        "  print(word+\"---->\"+lemmatizer.lemmatize(word, pos='v'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Pb-s0AKd1A",
        "outputId": "5d5fb57c-978b-4028-e3fe-4c31667bf1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going---->go\n",
            "goes---->go\n",
            "eaten---->eat\n",
            "went---->go\n",
            "everything---->everything\n",
            "people---->people\n",
            "notice---->notice\n",
            "finally---->finally\n",
            "finalized---->finalize\n",
            "programm---->programm\n",
            "programming---->program\n",
            "ate---->eat\n",
            "finalizing---->finalize\n",
            "history---->history\n",
            "historical---->historical\n",
            "fairly---->fairly\n",
            "strongly---->strongly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stopwords**"
      ],
      "metadata": {
        "id": "M6RFRVxgN6qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "#stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by1xNBYwN6GK",
        "outputId": "f43c4300-ceef-48a4-c405-6b5bb25cca33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"The Sun was shining brightly, and the birds were singing in the trees. It was a perfect day for a walk in the park! As she strolled along the path, she noticed children playing and people relaxing on benches. A gentle breeze carried the scent of blooming flowers through the air. Everything seemed just right.\""
      ],
      "metadata": {
        "id": "LtUrPez6Ys-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sent_tokenize(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkq7sjYCYsth",
        "outputId": "adb4350c-a39d-41b6-cdc3-4a30924d815b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Sun was shining brightly, and the birds were singing in the trees.',\n",
              " 'It was a perfect day for a walk in the park!',\n",
              " 'As she strolled along the path, she noticed children playing and people relaxing on benches.',\n",
              " 'A gentle breeze carried the scent of blooming flowers through the air.',\n",
              " 'Everything seemed just right.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with stemming\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words= [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words) #converting all the list of words into sentences\n",
        "sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3c5wXfPjYsWy",
        "outputId": "48bc702f-269c-4d31-a239-c726131fcf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the sun shine bright , bird sing tree .',\n",
              " 'it perfect day walk park !',\n",
              " 'as stroll along path , notic children play peopl relax bench .',\n",
              " 'a gentl breez carri scent bloom flower air .',\n",
              " 'everyth seem right .']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with lemmatizing\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    processed_words = []  # Create a new list to store lemmatized words\n",
        "    for word in words:\n",
        "        if word not in set(stopwords.words('english')):\n",
        "            processed_words.append(lemmatizer.lemmatize(word.lower(), pos='v'))  # Append to the new list\n",
        "    sentences[i] = ' '.join(processed_words)  # Join processed words to form the sentence\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c13hW9lokqgN",
        "outputId": "4045ad65-78b6-4010-bfd7-eddb46dd3fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the sun shin brightly , bird sing tree .',\n",
              " 'it perfect day walk park !',\n",
              " 'as stroll along path , notice children play people relax bench .',\n",
              " 'a gentle breeze carry scent bloom flower air .',\n",
              " 'everything seem right .']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}